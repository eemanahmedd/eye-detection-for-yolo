{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a37b04-d015-45c9-b485-f8d6fa1313b2",
   "metadata": {},
   "source": [
    "# eye detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e1ac8-7b87-41e6-8d72-79b0f900853a",
   "metadata": {},
   "source": [
    "# eye detection using mediapipe face detection model and face mesh model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd67c4d4-8019-44aa-ab2d-570dd12101fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 15:38:28.794438: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-29 15:38:29.025556: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-29 15:38:29.025600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-29 15:38:29.046659: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-29 15:38:29.071947: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-29 15:38:29.073714: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-29 15:38:30.505052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a479583-c56a-42c1-838f-437240df5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_and_append_new_bounding_boxes(original_file_path, new_file_path, all_bboxes):\n",
    "    \"\"\"\n",
    "    Copies an existing annotation file and appends new bounding boxes with class labels to the copy.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_file_path: Path to the original annotation file.\n",
    "    - new_file_path: Path for the new annotation file where the content is to be copied and new bounding boxes appended.\n",
    "    - new_bboxes: List of new bounding boxes to append, where each bounding box is \n",
    "                  represented as (class_label, x_center_norm, y_center_norm, width_norm, height_norm).\n",
    "    \"\"\"\n",
    "    # Copy the original file to a new location\n",
    "    shutil.copy(original_file_path, new_file_path)\n",
    "    \n",
    "    # Append new bounding boxes to the copied file\n",
    "    with open(new_file_path, 'a') as file:  # Open file in append mode\n",
    "        for new_bboxes in all_bboxes:\n",
    "            for bbox in new_bboxes:\n",
    "                line = ' '.join(map(str, bbox)) + '\\n'  # Convert each bbox to a string line\n",
    "                file.write(line)  # Append the new line to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91f76f08-c744-45bb-b06a-7a96560d8b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the bounding box from landmarks\n",
    "def calculate_bounding_box(landmarks, image_width, image_height):\n",
    "    x_coordinates = [landmark.x for landmark in landmarks]\n",
    "    y_coordinates = [landmark.y for landmark in landmarks]\n",
    "    \n",
    "    xmin = min(x_coordinates) * image_width\n",
    "    xmax = max(x_coordinates) * image_width\n",
    "    ymin = min(y_coordinates) * image_height\n",
    "    ymax = max(y_coordinates) * image_height\n",
    "    \n",
    "    return int(xmin), int(ymin), int(xmax), int(ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34924174-d282-4945-8490-b3ab8020eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_yolo_format(xmin, ymin, xmax, ymax, image_width, image_height):\n",
    "    x_center = (xmin + xmax) / 2\n",
    "    y_center = (ymin + ymax) / 2\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    \n",
    "    # Normalize\n",
    "    x_center_norm = x_center / image_width\n",
    "    y_center_norm = y_center / image_height\n",
    "    width_norm = width / image_width\n",
    "    height_norm = height / image_height\n",
    "    \n",
    "    return (x_center_norm, y_center_norm, width_norm, height_norm)\n",
    "\n",
    "# Function to convert eye bounding boxes to YOLO format\n",
    "def convert_eyes_to_yolo(left_eye_bbox, right_eye_bbox, image_width, image_height):\n",
    "    left_eye_yolo = convert_to_yolo_format(*left_eye_bbox, image_width, image_height)\n",
    "    right_eye_yolo = convert_to_yolo_format(*right_eye_bbox, image_width, image_height)\n",
    "    return left_eye_yolo, right_eye_yolo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669ee000-4483-45f4-9927-782bfe792db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709203114.187101   28021 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1709203114.225161   28086 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 440.33.01), renderer: GeForce GTX 1050 Ti/PCIe/SSE2\n",
      "I0000 00:00:1709203114.233384   28021 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1709203114.254774   28099 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 440.33.01), renderer: GeForce GTX 1050 Ti/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Initialize MediaPipe Face Detection and Face Mesh.\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection =1, min_detection_confidence=0.5)\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=100, refine_landmarks=True, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad17ca43-a02f-4e4f-8a90-a21f50234c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_eye_indices = [33, 133, 160, 158, 153, 144, 145, 153]\n",
    "left_eye_indices = [362, 384, 381, 382, 263, 373, 374, 380]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caf35d0b-c2e9-4afa-823a-59c9ef0197a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image.\n",
    "image_path = '' # add your image path\n",
    "image = cv2.imread(image_path)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ca289eb-4eeb-43e9-96cc-e06f2fd04899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect faces in the image.\n",
    "detections = face_detection.process(image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cd05e13-4518-429e-a5d6-c2b60fc0100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if detections.detections:\n",
    "    for detection in detections.detections:\n",
    "        bboxC = detection.location_data.relative_bounding_box\n",
    "        ih, iw, _ = image.shape\n",
    "        x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "        \n",
    "        padding = 0.1 # You can change the padding value to whatever suits your needs\n",
    "        x_padding = int(w * padding)\n",
    "        y_padding = int(h * padding)\n",
    "\n",
    "        # Modify x, y, w, and h to include padding, ensuring we don't go out of the image bounds\n",
    "        x = max(0, x - x_padding)\n",
    "        y = max(0, y - y_padding)\n",
    "        w = min(iw - x, w + 2 * x_padding)\n",
    "        h = min(ih - y, h + 2 * y_padding)\n",
    "\n",
    "        # Apply face mesh on the enlarged detected face region.\n",
    "        face_roi = image_rgb[y:y+h, x:x+w]\n",
    "        \n",
    "        # Apply face mesh on the detected face region.\n",
    "        face_roi = image[y:y+h, x:x+w]\n",
    "        \n",
    "        roi_h, roi_w, _ = face_roi.shape\n",
    "        mesh_results = face_mesh.process(cv2.cvtColor(face_roi, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        if mesh_results.multi_face_landmarks:\n",
    "            for face_landmarks in mesh_results.multi_face_landmarks:\n",
    "                left_eye = [face_landmarks.landmark[i] for i in left_eye_indices]  # Left eye horizontal boundary.\n",
    "                right_eye = [face_landmarks.landmark[i] for i in right_eye_indices]  # Right eye horizontal boundary.\n",
    "                \n",
    "                # Calculate bounding boxes within the face ROI.\n",
    "                left_eye_bbox = calculate_bounding_box(left_eye, roi_w, roi_h)\n",
    "                right_eye_bbox = calculate_bounding_box(right_eye, roi_w, roi_h)\n",
    "                # Offset the bounding box coordinates to the original image's coordinate space.\n",
    "                left_eye_bbox = (left_eye_bbox[0] + x, left_eye_bbox[1] + y, left_eye_bbox[2] + x, left_eye_bbox[3] + y)\n",
    "                right_eye_bbox = (right_eye_bbox[0] + x, right_eye_bbox[1] + y, right_eye_bbox[2] + x, right_eye_bbox[3] + y)\n",
    "\n",
    "                \n",
    "                # Convert and print the bounding boxes in YOLO format\n",
    "                left_eye_yolo, right_eye_yolo = convert_eyes_to_yolo(left_eye_bbox, right_eye_bbox, iw, ih)\n",
    "                # Add class labels to the tuples\n",
    "                right_eye_with_class = (2,) + right_eye_yolo  # Class 2 for right eye\n",
    "                left_eye_with_class = (2,) + left_eye_yolo  # Class 2 for left eye\n",
    "\n",
    "                # List of new bounding boxes to append\n",
    "                new_bboxes = [right_eye_with_class, left_eye_with_class]\n",
    "                # print(left_eye_bbox, left_eye_with_class)\n",
    "                # print(right_eye_bbox, right_eye_with_class)\n",
    "                \n",
    "                # Call the function to copy the original file and append new bounding boxes\n",
    "                # copy_and_append_new_bounding_boxes(label_path, new_file_path, new_bboxes)\n",
    "                cv2.rectangle(image, (int(left_eye_bbox[0]), int(left_eye_bbox[1])), (int(left_eye_bbox[2]), int(left_eye_bbox[3])), (0, 255, 0), 2)\n",
    "                cv2.rectangle(image, (int(right_eye_bbox[0]), int(right_eye_bbox[1])), (int(right_eye_bbox[2]), int(right_eye_bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('Eyes Bounding Boxes', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9ce1e-17ec-481c-908c-c85874627d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
